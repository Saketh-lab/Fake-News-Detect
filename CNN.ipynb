{"cells":[{"cell_type":"markdown","metadata":{"id":"TiM6gYg0nhkY"},"source":["<font color=\"#de3023\"><h1><b>REMINDER MAKE A COPY OF THIS NOTEBOOK, DO NOT EDIT</b></h1></font>"]},{"cell_type":"markdown","metadata":{"id":"Sq-JhCcLpBwS"},"source":["#Conscientious Cars 2: Convolutional Neural Nets\n","\n","Welcome back to CC: ConscientiousCars! Today, we'll be improving on our system for distinguishing dogs from roads."]},{"cell_type":"code","execution_count":2,"metadata":{"id":"uhNVum16scIW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1690648375031,"user_tz":-330,"elapsed":18040,"user":{"displayName":"sreesaketh k","userId":"07555302498620567571"}},"outputId":"82e5c08c-757a-4b2f-fe79-f815321c1c62"},"outputs":[{"output_type":"stream","name":"stdout","text":["  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for scikeras (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["#@title Run this to load some packages and data! { display-mode: \"form\" }\n","import pickle\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn import model_selection\n","from sklearn.metrics import accuracy_score\n","from collections import Counter\n","import keras.api._v2.keras as keras\n","from keras.models import Sequential\n","from keras.layers import Dense, Conv2D, InputLayer\n","from keras.layers import Activation, MaxPooling2D, Dropout, Flatten, Reshape\n","from keras.utils.np_utils import to_categorical\n","!pip install -q git+https://github.com/rdk2132/scikeras # workaround for scikeras deprecation\n","from scikeras.wrappers import KerasClassifier\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.model_selection import cross_val_score\n","\n","# Quiet deprecation warnings\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","def categorical_to_numpy(labels_in):\n","  labels = []\n","  for label in labels_in:\n","    if label == 'dog':\n","      labels.append(np.array([1, 0]))\n","    else:\n","      labels.append(np.array([0, 1]))\n","  return np.array(labels)\n","\n","def load_data():\n","  # import the data from the Cloud\n","  !wget -q --show-progress https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%201%20-%205/Session%204%20_%205%20-%20Neural%20Networks%20_%20CNN/dogs_v_roads\n","\n","  # initialize our input and output variables\n","  data_dict = pickle.load(open( \"dogs_v_roads\", \"rb\" ));\n","  data   = data_dict['data']\n","  labels = data_dict['labels']\n","\n","  return data, labels\n","\n","\n","def plot_one_image(data, labels, img_idx):\n","  from google.colab.patches import cv2_imshow\n","  import cv2\n","  import matplotlib.pyplot as plt\n","  my_img   = data[img_idx, :].squeeze().reshape([32,32,3]).copy()\n","  my_label = labels[img_idx]\n","  print('label: %s'%my_label)\n","  plt.imshow(my_img)\n","  plt.show()\n","\n","def CNNClassifier(num_epochs=2, layers=1, dropout=0.15):\n","  def create_model():\n","    model = Sequential()\n","    model.add(Reshape((32, 32, 3)))\n","\n","    for i in range(layers):\n","      model.add(Conv2D(32, (3, 3), padding='same'))\n","      model.add(Activation('relu'))\n","\n","    model.add(Conv2D(32, (3, 3)))\n","    model.add(Activation('relu'))\n","    model.add(MaxPooling2D(pool_size=(2, 2)))\n","    model.add(Dropout(dropout))\n","\n","    model.add(Conv2D(64, (3, 3), padding='same'))\n","    model.add(Activation('relu'))\n","    model.add(Conv2D(64, (3, 3)))\n","    model.add(Activation('relu'))\n","    model.add(MaxPooling2D(pool_size=(2, 2)))\n","    model.add(Dropout(dropout))\n","\n","    model.add(Flatten())\n","    model.add(Dense(512))\n","    model.add(Activation('relu'))\n","    model.add(Dropout(dropout))\n","    model.add(Dense(2))\n","    model.add(Activation('softmax'))\n","\n","    return model\n","  # initiate RMSprop optimizer\n","  opt = keras.optimizers.legacy.RMSprop(learning_rate=0.0001, decay=1e-6)\n","  return KerasClassifier(model=create_model, optimizer=opt, loss='categorical_crossentropy', epochs=num_epochs, batch_size=10, verbose=2, validation_batch_size=10, validation_split=.2, metrics=['accuracy'])\n","\n","\n","def plot_acc(history, ax = None, xlabel = 'Epoch #'):\n","\n","    if hasattr(history, 'history_'):\n","      history = history.history_\n","    else:\n","      history = history.history\n","    history.update({'epoch':list(range(len(history['val_accuracy'])))})\n","    history = pd.DataFrame.from_dict(history)\n","\n","    best_epoch = history.sort_values(by = 'val_accuracy', ascending = False).iloc[0]['epoch']\n","\n","    if not ax:\n","      f, ax = plt.subplots(1,1)\n","    sns.lineplot(x = 'epoch', y = 'val_accuracy', data = history, label = 'Validation', ax = ax)\n","    sns.lineplot(x = 'epoch', y = 'accuracy', data = history, label = 'Training', ax = ax)\n","    ax.axhline(0.5, linestyle = '--',color='red', label = 'Chance')\n","    ax.axvline(x = best_epoch, linestyle = '--', color = 'green', label = 'Best Epoch')\n","    ax.legend(loc = 7)\n","    ax.set_ylim([0.4, 1])\n","\n","    ax.set_xlabel(xlabel)\n","    ax.set_ylabel('Accuracy (Fraction)')\n","\n","    plt.show()\n","\n","def model_to_string(model):\n","    import re\n","    stringlist = []\n","    model.summary(print_fn=lambda x: stringlist.append(x))\n","    sms = \"\\n\".join(stringlist)\n","    sms = re.sub('_\\d\\d\\d','', sms)\n","    sms = re.sub('_\\d\\d','', sms)\n","    sms = re.sub('_\\d','', sms)\n","    return sms"]},{"cell_type":"markdown","metadata":{"id":"wkseZ14ms7vs"},"source":["In this notebook, we will:\n","\n","- Use a pre-built CNN function to classify roads vs. dogs.\n","- Build neural networks from scratch in Keras.\n","- Experiment with building CNN models from scratch in Keras.\n","- (Advanced, Optional) Build CNN models for distinguishing cats from dogs, and even experiment with implementing a famous architecture!\n"]},{"cell_type":"markdown","metadata":{"id":"qMWQRlTqt6Yn"},"source":["<font color=darkorange>**Change Hardware Accelerator to GPU to train faster (Runtime -> Change Runtime Type -> Hardware Accelerator -> GPU)**"]},{"cell_type":"markdown","metadata":{"id":"1QxGsnvhnn8R"},"source":["#Loading in Data"]},{"cell_type":"markdown","metadata":{"id":"btr24O6Hqgo6"},"source":["Once again, let's load in our dog/road dataset and create our training and test set. **What's the shape of each dataset? Why?**"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"MmZbrZoKnthN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1690648390938,"user_tz":-330,"elapsed":652,"user":{"displayName":"sreesaketh k","userId":"07555302498620567571"}},"outputId":"8e040af7-4648-45f6-e936-f466ec0acdb9"},"outputs":[{"output_type":"stream","name":"stdout","text":["\rdogs_v_roads          0%[                    ]       0  --.-KB/s               \rdogs_v_roads        100%[===================>]   3.52M  --.-KB/s    in 0.04s   \n"]},{"output_type":"execute_result","data":{"text/plain":["(1200, 3072)"]},"metadata":{},"execution_count":3}],"source":["# load our data\n","data_raw, labels_raw = load_data()\n","data = data_raw.astype(float)\n","labels = categorical_to_numpy(labels_raw)\n","inputs_train, inputs_test, labels_train, labels_test = model_selection.train_test_split(data, labels, test_size=0.2, random_state=1)\n","\n","#Find the shape of our dataset!\n","### YOUR CODE HERE\n","data.shape\n","### END CODE"]},{"cell_type":"markdown","metadata":{"id":"VcZra2S0NNSZ"},"source":["Use the cell below as a reminder of what the data looks like."]},{"cell_type":"code","execution_count":4,"metadata":{"id":"B83F3CmPNSux","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1690648392232,"user_tz":-330,"elapsed":1297,"user":{"displayName":"sreesaketh k","userId":"07555302498620567571"}},"outputId":"af9e5423-ce58-480f-c87c-3ac1f9d7bea5"},"outputs":[{"output_type":"stream","name":"stdout","text":["label: dog\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}],"source":["plot_one_image(data_raw, labels_raw, 300) # change this number"]},{"cell_type":"markdown","metadata":{"id":"37O_VE_D1Bdy"},"source":["# Models for Vision: Convolutional Neural Networks"]},{"cell_type":"markdown","metadata":{"id":"GqrfI4JiVeFr"},"source":["###Exercise: Exploring Hyperparameters ✍️\n","\n","As you know, there is a famous type of neural network known as convolutional neural networks (CNNs). These types of neural networks work particularly well on problems to do with computer vision. Let's try one out!\n","\n","To load up a simple CNN, just run:\n","\n","`cnn = CNNClassifier(num_epochs, layers, dropout)`\n","\n","Work with your instructors to review what each parameter means and how it affects the model! The value for **dropout** is a float between 0 and 1 that represents the probability the weight for a neuron in the layer is set to 0 during training time. Each neuron in the layer is evaluated as such, which can help prevent overfitting.\n","\n","**Try different values of num_epochs, layers, and dropout so that you get the best possible accuracy on the test set using `model.score()`**!"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"bmC3-T4KRJgW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1690648417804,"user_tz":-330,"elapsed":22908,"user":{"displayName":"sreesaketh k","userId":"07555302498620567571"}},"outputId":"84ad7d16-de7d-4ec5-9270-4fbfd04f3bcc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","77/77 - 15s - loss: 6.2966 - accuracy: 0.5911 - val_loss: 0.4448 - val_accuracy: 0.8542 - 15s/epoch - 192ms/step\n","Epoch 2/5\n","77/77 - 0s - loss: 1.3984 - accuracy: 0.7201 - val_loss: 0.2728 - val_accuracy: 0.8854 - 458ms/epoch - 6ms/step\n","Epoch 3/5\n","77/77 - 0s - loss: 0.5641 - accuracy: 0.8281 - val_loss: 0.2571 - val_accuracy: 0.8854 - 446ms/epoch - 6ms/step\n","Epoch 4/5\n","77/77 - 1s - loss: 0.3943 - accuracy: 0.8724 - val_loss: 0.2314 - val_accuracy: 0.8906 - 515ms/epoch - 7ms/step\n","Epoch 5/5\n","77/77 - 1s - loss: 0.3486 - accuracy: 0.8867 - val_loss: 0.2268 - val_accuracy: 0.9010 - 610ms/epoch - 8ms/step\n","24/24 - 0s - 258ms/epoch - 11ms/step\n","0.9375\n"]}],"source":["cnn = CNNClassifier(5, 2, 0.5)\n","cnn.fit(inputs_train, labels_train)\n","print(cnn.score(inputs_test, labels_test))"]},{"cell_type":"markdown","metadata":{"id":"KGWpgsVXP1ut"},"source":["**How well did your neural network perform?**\n","\n","CNNs typically perform better than fully-connected neural networks on vision problems, but, as before, they aren't always consistent. They are also sensitive to a number of parameters."]},{"cell_type":"markdown","metadata":{"id":"c-XRh5Y5P_CL"},"source":["## Training and Validation Curves\n","\n","An important aspect of training neural networks is to prevent overfitting. **How would we recognize overfitting?**\n","\n","In the first line of code below, we first **fit** the model on the training data and pass in some validation (or test) data to evaluate it. We call it the **history** because we want to retain information about the accuracy at each epoch.\n","\n","In the second line we plot the history so that we can compare the training and validation accuracies.  \n","\n","```\n","history = model.fit(inputs_train, labels_train, validation_data=(inputs_test, labels_test))\n","plot_acc(history)\n","```"]},{"cell_type":"markdown","metadata":{"id":"8eaFvE2PQEFe"},"source":["###Exercise: Plotting a Training vs. Validation Curve For Our CNN ✍️\n","\n","**After how many epochs does the model begin to overfit? How does this vary as you vary the number of hidden layers and dropout?** Overfitting occurs when the validation accuracy starts to drop below the training accuracy."]},{"cell_type":"code","execution_count":6,"metadata":{"id":"OsVAasDbjARJ","executionInfo":{"status":"ok","timestamp":1690649149916,"user_tz":-330,"elapsed":371,"user":{"displayName":"sreesaketh k","userId":"07555302498620567571"}}},"outputs":[],"source":["### YOUR CODE HERE\n","\n","### END CODE"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"zEYUz17TJCGX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1690649152215,"user_tz":-330,"elapsed":5,"user":{"displayName":"sreesaketh k","userId":"07555302498620567571"}},"outputId":"59776658-6621-42dd-ac45-f452f36a5b39"},"outputs":[{"output_type":"stream","name":"stdout","text":["CNN Testing Set Score:\n","24/24 - 0s - 58ms/epoch - 2ms/step\n","0.9375\n"]}],"source":["# Print the score on the testing data\n","print(\"CNN Testing Set Score:\")\n","print(cnn.score(inputs_test, labels_test))"]},{"cell_type":"markdown","metadata":{"id":"76z4NAY6afd7"},"source":["# Building Neural Networks from Scratch in Keras\n","\n","So far, we've used helper functions which pre-build Keras neural network models. Now, we will build them on our own!\n","\n","Let's start with a \"toy example\": a tiny neural network with just three numerical inputs.\n"]},{"cell_type":"markdown","metadata":{"id":"jdngkAX_aCVu"},"source":["###Exercise: Building a Simple Neural Network Using Keras! ✍️\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Bj-Pt3wGCXRu"},"source":["\n","We're going to build this model:\n","\n","![](http://cs231n.github.io/assets/nn1/neural_net.jpeg)"]},{"cell_type":"markdown","metadata":{"id":"H-6WGeedvTCS"},"source":["This network can be described as:\n","* Input Layer: 3 neurons\n","* Layer 1 (Hidden): 4 neurons that are activated by `'relu'`\n","* Layer 2 (Output): 2 neurons that are activated by `'softmax'`\n","\n","\n","We also want to compile the model with\n","`loss = 'categorical_crossentropy'`\n","\n","What does this represent? Here's one way to interpret it:\n","* This model classifies animals as \"cat\" or \"dog\"\n","* Our three inputs are height, weight, and age\n","* Our ouputs represent \"probability of cat\" and \"probability of dog\"\n","* Because this is a toy example, we aren't actually training the model here - just using randomly initialized weights! We will train later models in this notebook.\n","\n","Try filling in the blanks below and walking through each line! **If you want a hint or more details, check out the optional reference below.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dp-g9qotbRPU"},"outputs":[],"source":["# Fill in the blanks with your group!\n","### YOUR CODE HERE:\n","model_1 = Sequential()\n","model_1.add(InputLayer(input_shape=(____,)))\n","model_1.add(Dense(____, activation = '____'))\n","model_1.add(Dense(____, activation = '____'))\n","model_1.compile(loss='____',\n","                optimizer = 'adam',\n","                metrics = ['accuracy'])\n","model_1.predict([[14,18,5]]) #Try any input! This represents an animal of height 14, weight 18, and age 5.\n","### END CODE"]},{"cell_type":"markdown","metadata":{"id":"baWZ9zXtF2Cz"},"source":["**Discuss:** How would you interpret this output? Does our (untrained) network classify this as a cat or a dog?"]},{"cell_type":"markdown","metadata":{"id":"781M4IyhssuA"},"source":["####**Optional Reference**\n","\n","Here's some information about each step of the process. **You don't need to read through all this - check it as a reference if needed!**\n","\n","**1. Specify model**\n","\n","```\n","model = Sequential()\n","```\n","In this line of code, we build our network where the information flows from LEFT to RIGHT through the network in ONE DIRECTION as opposed to multiple directions. Neurons on the right never pass informations to neurons on the left of it.\n","\n","\n","**2. Add layers to the network**\n","```\n","model.add(Dense(4, activation = 'sigmoid'))\n","```\n","In this code, we add a layer of neurons to our network.\n","\n","This layers consists of 4 neurons. Each neuron is DENSE and connects to all of the previous layer's inputs and all of the subsequent layers outputs. We specify that there are 3 inputs here.\n","\n","We also specify what kind of output the neuron will give. If you want the neuron to output a number between 0 and 1 (like a probability!) you would use `'softmax'` or `'sigmoid'`. If you want the neuron to output any number, you can use `'linear'`! You'll also often see `'relu'`, which is when a neuron will only output positive numbers.\n","\n","```\n","model.add(Dense(1, activation = 'linear'))\n","```\n","This code adds ANOTHER layer to the network that has 1 neuron. This one neuron is used to predict a continuous value!\n","\n","**3. Turn the model on by compiling it**\n","\n","After having built the network, we want to train and use it, so we have to 'compile' it to prepare. We have to specify at the very least: a loss (how the model measures the quality of its weights), an optimizer (which adjusts the weights), and a metric (how to evaluate our results). Here are some common choices:\n","```\n","model.compile(loss='mean_squared_error',\n","optimizer = 'adam',\n","metrics = ['mean_squared_error'])\n","  ```\n","\n","Once we've created our network, we can use it very simply! Just like we did with sklearn, we define our input data (x), the true predictions from that data (y), and then train our model with `fit`.\n","\n","```\n","model.fit(x, y)\n","```\n","\n","To use the model, you can use it to predict something with:\n","```\n","y = model.predict(x)\n","```\n","\n","You can actually use the model before you even train it! It just won't perform very well."]},{"cell_type":"markdown","metadata":{"id":"YovNRgfuy0Oq"},"source":["###(Optional) Exercise: Building a Multi-layer Neural Net Using Keras ✍️\n","\n","![](http://cs231n.github.io/assets/nn1/neural_net2.jpeg)\n","\n","Let's try another, bigger example!\n","\n","Here, we are predicting a house price: regression! Our inputs could be \"year the house was built\", \"home square footage\", and \"lot square footage\", while our output is price (in thousands of dollars).\n","\n","* Input Layer: 3 neurons\n","\n","* Layer 1: 4 neurons that are activated by `'relu' `\n","\n","* Layer 2: 4 neurons that are activated by `'relu'`\n","\n","* Layer 3 (out): 1 neuron that is activated by `'relu'`\n","\n","Compile the model with\n","`'mean_squared_error'` as both loss and metric, and try making a prediction for some made-up data.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pm-ylEWqbXrQ"},"outputs":[],"source":["### YOUR CODE HERE\n","\n","### END CODE"]},{"cell_type":"markdown","metadata":{"id":"YVCmntRHRjPQ"},"source":["###(Optional) Exercise: Dogs vs. Roads Using Keras\n","\n","Let's try an even bigger example! Here, we are going to distinguish between images of dogs and roads once again.\n","\n","* Input Layer: 3072 dimensions (32 pixels x 32 pixels x 3 color channels)\n","\n","* Layer 1: 32 neurons that are activated by `'relu' `and take in 3072 inputs.\n","\n","* Layer 2: 16 neurons that are activated by `'relu'`\n","\n","* Layer 3 (out): 2 neurons that are activated by `'softmax'`\n","\n","Compile the model with\n","`loss = 'categorical_crossentropy'`, and try making predictions on `inputs_train`!\n","\n","Once again, we are not actually training this model - so the predictions won't be any good. Soon we will create a CNN, which we will train!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DD2Dc4AYR31r"},"outputs":[],"source":["### YOUR CODE HERE\n","\n","### END CODE"]},{"cell_type":"markdown","metadata":{"id":"QCbD6siv-Ip-"},"source":["##Exercise: Building a CNN Using Keras! ✍️\n","\n","Now that we know how to build simple neural networks in Keras, let's build a CNN! The CNN will perform well on our data set of car and road images.\n","\n","Below is Keras code for a CNN. It will run as-is on the conscientious cars dataset. However, the performance is suboptimal. Add more layers and change the neural network hyperparameters so that the performance will be better. **Can you get the train and validation accuracy to both be higher than 95%?**\n","\n","The Keras core layer API may be a useful reference: https://keras.io/layers/core/\n","\n","In particular and in addition to adding more of the existing convolutional layers and activations, consider using the following layers after a convolution + activation:\n","\n","`Dropout(N)`\n","\n","`MaxPooling2D(pool_size=(N, N))`\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LFVHyPKn-V4N"},"outputs":[],"source":["model = Sequential()\n","model.add(Reshape((32, 32, 3)))\n","\n","model.add(Conv2D(32, (3, 3), padding='same'))\n","model.add(Activation('relu'))\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","\n","###\n","###\n","### TODO: ADD MORE LAYERS HERE!!!!!\n","###\n","###\n","\n","model.add(Flatten())\n","model.add(Dense(512))\n","model.add(Activation('relu'))\n","model.add(Dense(2))\n","model.add(Activation('softmax'))\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(learning_rate=0.0001, decay=1e-6)\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","# Train the CNN and plot accuracy.\n","history = model.fit(inputs_train, labels_train, \\\n","                    validation_data=(inputs_test, labels_test), \\\n","                    epochs=70)\n","plot_acc(history)\n"]},{"cell_type":"markdown","metadata":{"id":"9JFLu0CdXM6K"},"source":["**What interesting observations** do you make from the graph? How many epochs should you train for?\n","\n","We can also print out the structure of our model. What do the parts of the summary mean?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RGwXs3C8YZl-"},"outputs":[],"source":["model.summary()"]},{"cell_type":"markdown","metadata":{"id":"YXINPAJvRr9W"},"source":["#Advanced: Cats vs. Dogs with CNN\n","\n","So far, we've trained a CNN to distinguish between small images of cats and small images of dogs. It's more challenging and time-consuming to train CNNs for bigger images or harder tasks, like distinguishing dogs from cats (which look a lot more like dogs than roads do!)\n","\n","In this exercise, you'll adapt your previous model to classify large images of dogs vs. cats, and then try implementing a famous CNN architecture. Along the way, you'll deal with some of the debugging that machine learning engineers often have to handle."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6gU39z3jNMAt"},"outputs":[],"source":["#@title Run this to load cat and dog data. { display-mode: \"form\" }\n","\n","#Code here from https://colab.research.google.com/github/google/eng-edu/blob/master/ml/pc/exercises/image_classification_part1.ipynb#scrollTo=4PIP1rkmeAYS\n","\n","import tensorflow as tf\n","import os\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from google.colab.patches import cv2_imshow\n","import cv2\n","import matplotlib.pyplot as plt\n","\n","try:\n","  road_model = model\n","  road_saved = True\n","except NameError:\n","  road_saved = False\n","\n","IMG_SHAPE  = 150  # Our training data consists of images with width of 150 pixels and height of 150 pixels\n","_URL = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'\n","zip_dir = tf.keras.utils.get_file('cats_and_dogs_filterted.zip', origin=_URL, extract=True)\n","base_dir = os.path.join(os.path.dirname(zip_dir), 'cats_and_dogs_filtered')\n","train_dir = os.path.join(base_dir, 'train')\n","validation_dir = os.path.join(base_dir, 'validation')\n","\n","train_cats_dir = os.path.join(train_dir, 'cats')  # directory with our training cat pictures\n","train_dogs_dir = os.path.join(train_dir, 'dogs')  # directory with our training dog pictures\n","validation_cats_dir = os.path.join(validation_dir, 'cats')  # directory with our validation cat pictures\n","validation_dogs_dir = os.path.join(validation_dir, 'dogs')  # directory with our validation dog pictures\n","train_image_generator      = ImageDataGenerator()  # Generator for our training data\n","validation_image_generator = ImageDataGenerator()  # Generator for our validation data\n","train_data = train_image_generator.flow_from_directory(batch_size=2000,\n","                                                           directory=train_dir,\n","                                                           shuffle=True,\n","                                                           target_size=(IMG_SHAPE,IMG_SHAPE), #(150,150)\n","                                                           class_mode='binary').next()\n","val_data = validation_image_generator.flow_from_directory(batch_size=1000,\n","                                                              directory=validation_dir,\n","                                                              shuffle=False,\n","                                                              target_size=(IMG_SHAPE,IMG_SHAPE), #(150,150)\n","\n","                                                              class_mode='binary').next()\n","cd_train_inputs, cd_train_labels = train_data\n","cd_test_inputs, cd_test_labels = val_data"]},{"cell_type":"markdown","metadata":{"id":"1y5etOJwScaG"},"source":["**Run the code below to see the dimensions of our training and validation data. What does each number mean? What is different than our previous dataset?**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kjdedJ0VNvWg"},"outputs":[],"source":["print(cd_train_inputs.shape)\n","print(cd_train_labels.shape)\n","print(cd_test_inputs.shape)\n","print(cd_test_labels.shape)"]},{"cell_type":"markdown","metadata":{"id":"qIAkgOqWTAL7"},"source":["**Run this code to see a random image from our training data (different each time).**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HooiJ-RrQPcA"},"outputs":[],"source":["index = np.random.randint(len(cd_train_inputs))\n","plt.imshow(cd_train_inputs[index]/255)\n","plt.show()\n","print(\"Label:\",cd_train_labels[index])"]},{"cell_type":"markdown","metadata":{"id":"HOwP9kX9UshH"},"source":["**By adapting code from the previous exercise, build, train, and test a CNN to classify cats vs. dogs.**\n","**Hints:**\n","*   Use `print(model.summary())` for a useful visualization of your model's architecture. Compare the summary of your cat/road and cat/dog classifiers.\n","*  Substitute the names of the new datasets.\n","*  Get a \"first try\" working by making small adjustments to a previous model before trying to optimize the accuracy. You can temporarily comment out layers as you figure things out.\n","*  The outputs have different shapes betweeen the two datasets. What do you need to change? (You will get an ValueError that suggests how to transform the output to a one-hot encoding.)\n","*  If you run out of memory, restart the notebook and/or use your knowledge of convolution arithmetic to reduce the size of an intermediate output (see [Keras documentation](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D)).\n","* Dropout layers help reduce overfitting.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AeuqlzigZZ8I"},"outputs":[],"source":["model = Sequential()\n","# TODO: Your code here to build, train, and test a cats vs. dogs CNN! (If you run into errors, see the hints above for help debugging!)\n"]},{"cell_type":"markdown","metadata":{"id":"m6sFSGEqjPwe"},"source":["#Advanced Challenge: Implementing a Famous Architecture for Cats vs. Dogs\n","\n","Having trouble designing an effective architecture? Try implementing a version of AlexNet, one of the most famous CNNs for image convolution ever. You can find this image and other useful information on this network [here](https://towardsdatascience.com/the-w3h-of-alexnet-vggnet-resnet-and-inception-7baaaecccc96).\n","\n","![](https://lh4.googleusercontent.com/gFAxn9Z-Y1lgkNy2GfsqjXy1DvSuYF8rvP3CslRvmuoP5SUaJMrEOr24YShU_LwalLpYNJFwpJgcDh9whk9XrMOGQ1ADQ9FY_0saicCVH0jsNPDKOYBcTG4YhbqpbPolW4hZSdUsDQ)\n","\n","How do we read this diagram?\n","\n","On the left side, we start with images of dimension 227x227x3 (RGB). We apply a filter composed of 96 kernels of size 11x11, with stride size 4. We end up with data of dimension 55x55x96. We pass through multiple layers of convolution and max pooling as shown, before ending with three dense (fully connected) layers.\n","\n","Not shown: each layer uses ReLU activation, and we include dropout before the first two dense layers. Make sure to include those!\n","\n","You'll want to adjust some of these dimensions, for a few reasons: we're starting with 150x150 rather than 227x227 images, ending with 2 labels rather than 1000, and have limited data and memory. Use your knowledge of convolution arithmetic (see CNN slides) and the [Keras documentation](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D) to change the stride, kernel, and/or padding.\n","\n","Use `model.summary()` to understand the dimensions of your data at each step. To speed things up as you're building, you can set the number of epochs to 1."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8FHg8YTGtQ2t"},"outputs":[],"source":["model = Sequential()\n","#TODO: Your code to run, train, and test AlexNet here:\n"]},{"cell_type":"markdown","metadata":{"id":"PlF308hDjwyC"},"source":["You might find that even AlexNet isn't working that well for you!\n","\n","This is because having a good architecture is only half the battle: AlexNet is a complex model designed to learn from millions of images. We're using a small dataset of only 2000 training images, so it's not surprising that our results aren't great. Our model is overfitting: essentially memorizing the few training images, rather than really learning the difference between a cat and a dog. (The advantage is that our model trains quickly.)\n","\n","To get really good performance, we need more data. If we can't find more, we could use *data augmentation*: inventing new training data by transforming our existing images. You can read more about it at https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html."]},{"cell_type":"markdown","source":["#From AlexNet to VGGNet: A Better Choice for Image Classification\n","\n","AlexNet's architecture may not be complex enough to capture the nuanced differences between cats and dogs, which might result in lower accuracy in classification tasks.\n","\n","VGGNet, or simply VGG, introduced by the Visual Geometry Group, is a much deeper model with its variants VGG16 and VGG19 having 16 and 19 layers respectively. VGG architecture has smaller, but more number of convolutional filters compared to AlexNet, allowing it to learn more complex features. This depth, along with the use of small 3x3 filters throughout the network, makes it excellent at learning hierarchical features in images, making it more suitable for our cat versus dog classification task.\n","\n","Moreover, VGG models have been trained on millions of images from the ImageNet\n","database, which includes a wide variety of animal images, making the learned features more generalized and better suited for our task. Hence, moving from AlexNet to VGG for our specific classification task would be a strategic choice to potentially improve our model's performance.\n","\n","So let's see how good it is at our task!"],"metadata":{"id":"ePRiJhGdEgQL"}},{"cell_type":"code","source":["#@title Run this to load images and imports! { display-mode: \"form\" }\n","#We have to resize the images to be 240,240 for the vgg net model.\n","\n","from keras.applications.vgg16 import VGG16\n","from keras.models import Model\n","from matplotlib import pyplot\n","from keras.utils import to_categorical\n","from keras.layers import BatchNormalization\n","from keras.optimizers import SGD\n","from keras.layers import Dense\n","from keras.layers import Flatten\n","\n","\n","train_data = train_image_generator.flow_from_directory(batch_size=2000,\n","                                                           directory=train_dir,\n","                                                           shuffle=True,\n","                                                           target_size=(224,224), #(150,150)\n","                                                           class_mode='binary').next()\n","val_data = validation_image_generator.flow_from_directory(batch_size=1000,\n","                                                              directory=validation_dir,\n","                                                              shuffle=False,\n","                                                              target_size=(224,224), #(150,150)\n","                                                              class_mode='binary').next()\n","cd_train_inputs, cd_train_labels = train_data\n","cd_test_inputs, cd_test_labels = val_data"],"metadata":{"id":"de8Dp3x9Eqgb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["It's crucial to note that VGG16 model requires input images to have a size of 224x224 pixels. We've already preprocessed our images to match this requirement, but ensure the model's input shape is set to this dimensionality as well!\n","\n","Also, you might have noticed that there's another dimension specified in the model's input shape. Wondering what that is? It's the channel dimension. For colored images, it is usually 3, representing the Red, Green, and Blue channels.\n","\n","Now, let's move onto our model configuration. For the first Dense layer, we will be using the 'relu' (Rectified Linear Unit) activation function. It's a commonly used activation function that introduces non-linearity in our model.\n","\n","Following that, we'll utilize the 'sigmoid' activation function in our final Dense layer. The sigmoid function squashes the output between the range of 0 and 1, making it suitable for binary classification tasks."],"metadata":{"id":"SxBngnvRFaDf"}},{"cell_type":"code","source":["# Fill in the underlines!\n","\n","model = VGG16(include_top=False, input_shape=(____,_____,_____))\n","\n","# mark loaded layers as not trainable\n","for layer in model.layers:\n","  layer.trainable = False\n","\n","flat1 = Flatten()(model.layers[-1].output)\n","class1 = Dense(128, activation='_____', kernel_initializer='he_uniform')(flat1)\n","output = Dense(2, activation='_______')(class1)\n","\n","\t# define new model\n","model = Model(inputs=model.inputs, outputs=output)\n","\n","# compile model\n","opt = SGD(lr=0.001, momentum=0.9)\n","model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n","\n","# Train the CNN and plot accuracy.\n","# Substituted new dataset names; to_categorical converts to one-hot, as ValueError suggests\n","history = model.fit(cd_train_inputs, to_categorical(cd_train_labels), \\\n","                    validation_data=(cd_test_inputs, to_categorical(cd_test_labels)), \\\n","                    epochs=2)\n","plot_acc(history)\n","\n","print(model.summary())"],"metadata":{"id":"bhkQWKVxEuGF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# You've reached the end!\n","Moving from AlexNet to VGGNet represents a shift towards more complex and powerful models. Don't feel discouraged if it feels overwhelming – the depth and complexity of VGGNet are part of its strength and why it performs so well.\n","\n","By experimenting with different hyperparameters, layers, and models you're not only enhancing your problem-solving skills but also your intuition about how neural networks operate.\n","\n","Remember, even if you don't hit 95% accuracy, the key is to learn from the process and understand why a certain setup works or why it doesn't.\n","\n","Keep up the fantastic work! Remember, the path to mastering machine learning is a marathon, not a sprint. Keep exploring, keep questioning, and most importantly, have fun with it!"],"metadata":{"id":"hRUZKDGPPWY_"}},{"cell_type":"markdown","metadata":{"id":"RVzEpI_xWpE5"},"source":["![](https://images.pexels.com/photos/316/black-and-white-animal-dog-pet.jpg?auto=compress&cs=tinysrgb&dpr=2&h=650&w=940)"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1vpdjHhpdsfxhkKx-6EyHGjTSB2F-B3P0","timestamp":1690648171991}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.2"}},"nbformat":4,"nbformat_minor":0}